CORE CONCEPTS
Pods
	Create a new pod with the name redis and the image redis123.  Use a pod-definition YAML file. And yes the image name is wrong!
	SOLUTION: kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml
	Edit YAML and apply
	
ReplicaSets
	Create a ReplicaSet using the replicaset-definition-1.yaml file located at /root/.  There is an issue with the file, so try to fix it.
	SOLUTION: You can just look at the YAML file.  You’ll see the apiVersion is wrong.  It should be ‘apps/v1’ for ReplicaSets
	ALSO:  kubectl api-resources - Gives you a list of API versions (Good to do kubectl api-resources | grep replicaset)
	ALSO:  kubectl explain replicaset - Doc on the YAML sections and what is needed.
	
	
	Fix the original replica set new-replica-set to use the correct busybox image.  Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.
	SOLUTION: kubectl edit replicaset new-replica-set
	Edit YAML (modify the name).  Delete the existing pods so they can recreate
	
	
	Scale the ReplicaSet to 5 PODs.  Use kubectl scale command or edit the replicaset using kubectl edit replicaset.
	SOLUTION: kubectl scale rs new-replica-set --replicas=5 OR kubectl edit rs new-replica-set (Edit YAML)


Deployments
	Create a new Deployment with the below attributes using your own deployment definition file.
	Name: httpd-frontend;
	Replicas: 3;
	Image: httpd:2.4-alpine
	SOLUTION: Two options.  1) Create the YAML.  2) Get a current deployment (if around) and use that yaml, copy, edit, apply.  ‘kubectl get deploy deployment-1 -o yaml > test.yaml
	DOC: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
	DOC Path: Concepts -> Workloads -> Workload Management -> Deployments
	YAML to build:
		---
			apiVersion: apps/v1
			kind: Deployment
			metadata:
  			  name: httpd-frontend
			spec:
  			  replicas: 3
  			  selector:
    			matchLabels:
      				name: httpd-frontend
 			 template:
    			metadata:
      			labels:
        			name: httpd-frontend
    			spec:
      			    containers:
      			- name: httpd-frontend
        		  image: httpd:2.4-alpine
	
Services
	What is the targetPort configured on the kubernetes service?
	SOLUTION: kubectl descrive svc kubernetes.  Look for the TargetPort field.
	
	Create a new service to access the web application using the service-definition-1.yaml file.
	Name: webapp-service
	Type: NodePort
	targetPort: 8080
	port: 8080
	nodePort: 30080
	selector:
  		name: simple-webapp
	SOLUTION: Two options.  1) Create the YAML.  2) Get a current deployment (if around) and use that yaml, copy, edit, apply.  ‘kubectl get svc kubernetes -o yaml > test.yaml
	DOC: https://kubernetes.io/docs/concepts/services-networking/service/
	DOC PATH: Concepts -> Services, Load Balancing, and Networking -> Service
	YAML to build:
			---
			apiVersion: v1
			kind: Service
			metadata:
  			  name: webapp-service
  			  namespace: default
			spec:
  			  ports:
  			  - nodePort: 30080
    			port: 8080
    			targetPort: 8080
  			selector:
    			name: simple-webapp
  			type: NodePort
	
	
	
Namespaces
Imperative Commands
	Deploy a redis pod using the redis:alpine image with the labels set to tier=db.  Either use imperative commands to create the pod with the labels. Or else use imperative commands to generate the pod definition file, then add the labels before creating the pod using the file.
	SOLUTION: Did it using the dry run YAML and editing the label.  But here’s the imperative command for the label.  kubectl get redis -l tier=db --image=redis:alpine
	
	
	Create a service redis-service to expose the redis application within the cluster on port 6379. Use imperative commands.
	SOLUTION: kubectl expose pod redis --port=6379 --name redis-service
	
	
	Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.  Try to use imperative commands only. Do not create definition files.
	SOLUTION: kubectl create deployment  webapp --image=kodekloud/webapp-color --replicas=3


	Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.  Try to do this with as few steps as possible.
	SOLUTION: kubectl run httpd --image=httpd:alpine --port=80 --expose


------------------------------------------------------------
SCHEDULING
Manual Scheduling
	Inspect the pod. Why is it in a pending state?
	SOLUTION: Check if the Pod is on a node.  It might not be.  ‘kubectl get pods -A -o wide’
	You can delete the pod and set the node manually by adjusting it’s yaml.  Add the nodeName field.
	DOC: https://kubernetes.io/docs/concepts/workloads/pods/
	DOC PATH: Concepts -> Workloads -> Pods
	YAML:
			apiVersion: v1
			kind: Pod
			metadata:
  			  name: nginx
			spec:
  			  nodeName: node01
 			  containers:
 			   -  image: nginx
     			  name: nginx
	
	
Labels and Selectors
	We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
	SOLUTION: kubectl get pods --selector env=dev
	
	Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
	SOLUTION: kubectl get all --selector env=prod,bu=finance,tier=frontend
	
	A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
	SOLUTION: The label under template has tier: nginx.  It should be tier:frontend
	DOC: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
	DOC PATH: Concetps -> Workloads -> Workload Management -> ReplicaSet
	YAML
		---
		apiVersion: apps/v1
		kind: ReplicaSet
		metadata:
   		  name: replicaset-1
		spec:
   		replicas: 2
   		selector:
      		matchLabels:
        		tier: front-end
   		template:
     	  metadata:
       		labels:
        	tier: front-end
     	  spec:
       		containers:
       		- name: nginx
         	  image: nginx



Taints and Tolerations
	Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
	SOLUTION: kubectl taint nodes node01 spray=mortein:NoSchedule
	
	Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
	SOLUTION: Create a Yaml
	DOC: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
	DOC PATH: Concepts -> Scheduling, Preemption and Eviction -> Taints and Tolerations
	YAML
			---
			apiVersion: v1
			kind: Pod	
			metadata:
  			  name: bee
			spec:
  			  containers:
  			  - image: nginx
    			name: bee
  			  tolerations:
  			  - key: spray
    			value: mortein
    			effect: NoSchedule
    			operator: Equal

	Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
	SOLUTION: kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-


Node Affinity
	Apply a label color=blue to node node01
	SOLUTION: kubectl label node node01 color=blue
	
	Which nodes can the pods for the blue deployment be placed on?
	SOLUTION: Look at each node to see if it has any active taints.  ‘kubectl describe node controlplane’
	
	Set Node Affinity to the deployment to place the pods on node01 only.
	Name: blue
	Replicas: 3
	Image: nginx 
	NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
	Key: color
	value: blue
	SOLUTION: Edit the deployment.  Add the affinity section to the containers:
	DOC: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
	DOC PATH: Concepts -> Scheduling, Preemption and Eviction -> Assigning Pods to Nodes
	YAML
	spec:
		replicas: 3
		selector:
		template:
			spec:
				containers:
				affinity:
					nodeAffinity:
          				requiredDuringSchedulingIgnoredDuringExecution:
            				nodeSelectorTerms:
            				- matchExpressions:
              				   - key: color
                				 operator: In
                				 values:
                				 - blue
	
	Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.  Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.
	Name: red
	Replicas: 2
	Image: nginx
	NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
	Key: node-role.kubernetes.io/control-plane
	Use the right operator
	SOLUTION: Create a YAML
	DOC: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
	DOC PATH: Concepts -> Scheduling, Preemption and Eviction -> Assigning Pods to Nodes
	YAML:
			---
			apiVersion: apps/v1
			kind: Deployment
			metadata:
  			  name: red
			spec:
  			  replicas: 2
  			  selector:
    			matchLabels:
      				run: nginx
  			  template:
    			metadata:
      			  labels:
        			 run: nginx
    			spec:
      			  containers:
      			  - image: nginx
        			imagePullPolicy: Always
        			name: nginx
      			  affinity:
        			nodeAffinity:
          			  requiredDuringSchedulingIgnoredDuringExecution:
            			 nodeSelectorTerms:
            				- matchExpressions:
              				  - key: node-role.kubernetes.io/control-plane
                				  operator: Exists

Resource Limits
	The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
	SOLUTION: kubectl get pods elephant -o yaml > em.yaml (I did kubectl edit, which complicates it).  Edit the YAML and adjust the RAM Limit.
	Delete the current Pod.  Recreate.  Don’t just to do kubectl apply -f test.yaml
	
	
DaemonSets
	Deploy a DaemonSet for FluentD Logging.  Use the given specifications.
	Name: elasticsearch
	Namespace: kube-system
	Image: registry.k8s.io/fluentd-elasticsearch:1.20
	SOLUTION: An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml. Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet.


Static Pods
	How many static pods exist in this cluster in all namespaces?
	SOLUTION: kubectl get pods -A and look for those with -controlplane appended in the name
	
	What is the path of the directory holding the static pod definition files?
	SOLUTION:  Identify the config file.  ps -aux | grep /usr/bin/kubelet.  You may see the kublet config file is used in /var/lib/kublet/config.yaml.
	Next, lookup the value assigned for staticPodPath.  “grep -i staticpod /var/lib/kubelet/config.yaml”.  The path will show /etc/kubernetes/manifests
	
	Create a static pod named static-busybox that uses the busybox image and the command sleep 1000
	Name: static-busybox
	Image: busybox
	SOLUTION: kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml.
	kubectl apply -f static-busybox.yaml
	
	We just created a new static pod named static-greenbox. Find it and delete it.
	SOLUTION: See what node the pod is in.  kubectl get pods -A.
	ssh to that node and identify the path for static pods.  ps -ef | grep /usr/bin/kubelet.  Look for the directory where kubeconfig sits.
	Look for the static pod in that directory.  grep -i staticpod /var/lib/kubelet/config.yaml (or whatever directory the yaml is in)
	Now that you have the path, go to it and delete the yaml file.  Go back to controlplane and try to delete the pod in K8s.
	
Multiple Schedulers
	What is the name of the POD that deploys the default kubernetes scheduler in this environment?
	SOLUTION: kubectl get pods -n kube-system
	
	Deploy an additional scheduler to the cluster following the given specification.
	Use the manifest file provided at /root/my-scheduler.yaml.
	Name: my-scheduler
	Status: Running
	Correct image used?
	SOLUTION: Edit my-scheduler.yaml.  Basically, just ensure the image is set as registry.k8s.io/kube-scheduler:v1.29.0
	
	A POD definition file is given. Use it to create a POD with the new custom scheduler.
	File is located at /root/nginx-pod.yaml
	SOLUTION: Edit the nginx-pod.yaml file.  Add a schedulerName.
	DOC: https://kubernetes.io/docs/reference/scheduling/config/#multiple-profiles
	DOC PATH: Reference -> Scheduling -> Scheduler Configuration
	YAML
		apiVersion: v1 
		kind: Pod 
		metadata:
  		  name: nginx 
		spec:
  		  schedulerName: my-scheduler
  		  containers:
  		  - image: nginx
    		name: nginx
------------------
LOGGING & MONITORING
Monitor Cluster Components
	Identify the POD that consumes the most Memory(bytes) in default namespace.
	SOLUTION: kubectl top pod
	
	
Managing Application Logs

------------------
APPLICATION LIFESTYLE MANAGEMENT
Rolling Updates and Rollbacks
	Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2.  Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
	SOLUTION: Edit the deployment.  Save it.  The rollout will automatically occur.  kubectl edit deployment frontend and modify the image to webapp:v2 using v2 as the new version from docker.
	

Commands and Arguments
	Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.
	SOLUTION: Edit the YAML provided
	DOC: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#define-a-command-and-arguments-when-you-create-a-pod
	DOC PATH: Tasks -> Inject Data into Applications -> Define a command and arguments for a container
	YAML:
		---
		apiVersion: v1 
		kind: Pod 
		metadata:
  		  name: ubuntu-sleeper-2 
		spec:
  		  containers:
  		  - name: ubuntu
    		image: ubuntu
    		command:
      		  - "sleep"
      		  - "5000"
      		  
	Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
	Pod Name: webapp-green
	Image: kodekloud/webapp-color
	Command line arguments: --color=green
	SOLUTION:  Create a YAML
	DOC: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#define-a-command-and-arguments-when-you-create-a-pod
	DOC PATH: Tasks -> Inject Data into Applications -> Define a command and arguments for a container
	YAML:
		---
		apiVersion: v1 
		kind: Pod 
		metadata:
  		  name: webapp-green
  		  labels:
      		name: webapp-green 
		spec:
  		  containers:
  		  - name: simple-webapp
    		image: kodekloud/webapp-color
    		args: ["--color", "green"]
	
		
Env Variables
	Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap. 
	SOLUTION: kubectl get pods webapp-color -o yaml > test.yaml.  Then Delete the pod.  Edit the yaml
	DOC: https://kubernetes.io/docs/concepts/configuration/configmap/#using-configmaps-as-environment-variables
	DOC PATH: Concepts -> Configuration -> ConfigMaps
	YAML:  Change value: xxx to this configmap
	spec
	  containers:
	  - env:
	  	- name: APP_COLOR
	  	  valueFrom:
	  	  	  configMapKeyRef:
	  	  	  	name: <ConfigMapName>
			    key: APP_COLOR
			    
			    
			    
Secrets
	The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.
	Secret Name: db-secret
	Secret 1: DB_Host=sql01
	Secret 2: DB_User=root
	Secret 3: DB_Password=password123
	SOLUTION: kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
	
	Configure webapp-pod to load environment variables from the newly created secret.
	SOLUTION: Export the pod - kubectl get pods webapp -o yaml > em.yaml.  Add the secret to the YAML
	DOC: https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables
	DOC PATH: Tasks -> Inject Data Into Applications -> Distrubute Credentials Securely Using Secrets
	YAML:
			---
			apiVersion: v1 
			kind: Pod 
			metadata:
  			  labels:
    			name: webapp-pod
  			  name: webapp-pod
  			  namespace: default 
			spec:
  			  containers:
  			  - image: kodekloud/simple-webapp-mysql
    			imagePullPolicy: Always
    			name: webapp
    			envFrom:
    			- secretRef:
        			name: db-secret
	

Multi Container Pods
	Create a multi-container pod with 2 containers.
	Name: yellow
	Container 1 Name: lemon
	Container 1 Image: busybox
	Container 2 Name: gold
	Container 2 Image: redis
	SOLUTION:  Create a YAML
	DOC: https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#creating-a-pod-that-runs-two-containers
	DOC PATH: Tasks -> Access Applications in a Cluster -> Communicate Between Containers in the Same Pod Using a Shared Volume
	YAML:
		apiVersion: v1
		kind: Pod
		metadata:
  		  name: yellow
		spec:
  		  containers:
  		  - name: lemon
    		image: busybox
    		command:
      			- sleep
      			- "1000"
  		  - name: gold
    		image: redis
    		
    		
    The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.  The app, called ‘app’ is in the elastic-search namespace.
    SOLUTION: kubectl -n elastic-stack exec -it app -- cat /log/app.log
    
    
    Edit the pod in the elastic-stack namespace to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.  Only add a new container. Do not modify anything else. Use the spec provided below.
    Name: app
	Container Name: sidecar
	Container Image: kodekloud/filebeat-configured
	Volume Mount: log-volume
	Mount Path: /var/log/event-simulator/
	Existing Container Name: app
	Existing Container Image: kodekloud/event-simulator
    SOLUTION: Create a YAML.  Delete and re-create the pod.
	DOC: https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/
	DOC PATH: Concepts -> Workloads -> Pods -> Sidecar Containers
    YAML:  
		---
		apiVersion: v1
		kind: Pod
		metadata:
  		   name: app
  		   namespace: elastic-stack
  		   labels:
    		name: app
		spec:
  		  containers:
  		  - name: app
    		image: kodekloud/event-simulator
    		volumeMounts:
    		- mountPath: /log
      		  name: log-volume
  		    - name: sidecar
    		  image: kodekloud/filebeat-configured
    		  volumeMounts:
    		  - mountPath: /var/log/event-simulator/
      			name: log-volume
  			volumes:
  			- name: log-volume
    		  hostPath:
      			# directory location on host
      		  path: /var/log/webapp
      			# this field is optional
      		  type: DirectoryOrCreate


	
Init Containers
	Identify the pod that has an initContainer configured.
	SOLUTION:  kubectl describe pod blue.  You’ll see there’s an Init Container.

	
	Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
	SOLUTION:  Create a YAML.  Delete and re-create the pod.
	DOC: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#init-containers-in-use
	DOC PATH: Concepts -> Workloads -> Pods -> Init Containers
	YAML:
		---
		apiVersion: v1
		kind: Pod
		metadata:
  		  name: red
  		  namespace: default
		spec:
  			containers:
  			- command:
    		  - sh
    		  - -c
    		  - echo The app is running! && sleep 3600
    		  image: busybox:1.28
    		  name: red-container
  			initContainers:
  			- image: busybox
    		  name: red-initcontainer
    		  command: 
      			 - "sleep"
      			 - "20"
		
------------------
CLUSTER MAINTENANCE
OS Upgrades
	We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
	SOLUTION:  kubectl drain node01 --ignore-daemonsets

Cluster Upgrade Process
	What is the current version of the cluster?
	SOLUTION: kubectl get nodes
	
	What is the latest version available for an upgrade with the current version of the kubeadm tool installed?
	SOLUTION: kubeadm upgrade plan
	
	Upgrade the controlplane components to exact version v1.29.0.  Upgrade the kubeadm tool (if not already), then the controlplane components, and finally the kubelet. Practice referring to the Kubernetes documentation page.
	SOLUTION: Use the documentation.  Example using Ubuntu and upgrading to v1.29.  But here’s step by step. Go to /etc/apt/sources.list.d/kubernetes.list.  Ensure the .gpg is set as the version you want.  v1.28? v1.29?  Whatever
	# apt update
	# apt-cache madison kubeadm
	# apt-get install kubeadm=1.29.0-1.1
	# kubeadm upgrade plan v1.29.0
	# kubeadm upgrade apply v1.29.0
	# apt-get install kubelet=1.29.0-1.1
	# systemctl daemon-reload
	# systemctl restart kubelet
	# kubectl uncordon controlplane
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrading-control-plane-nodes
	DOC PATH: Tasks -> Administer a Cluster -> Administration with kubeadm -> Upgrading kubeadm clusters
	
	
	Upgrade the worker node to the exact version v1.29.0
	SOLUTION: Use the documentation.  Example using Ubuntu and upgrading to v1.29.  But here’s step by step. Go to /etc/apt/sources.list.d/kubernetes.list.  Ensure the .gpg is set as the version you want.  v1.28? v1.29?  Whatever
	# apt update
	# apt-cache madison kubeadm
	# apt-get install kubeadm=1.29.0-1.1
	# kubeadm upgrade node
	# apt-get install kubelet=1.29.0-1.1
	# systemctl daemon-reload
	# systemctl restart kubelet
	# kubectl uncordon node01
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrade-worker-nodes
	DOC PATH: Tasks -> Administer a Cluster -> Administration with kubeadm -> Upgrading kubeadm clusters


Backup and Restore Methods
	The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.  Store the backup file at location /opt/snapshot-pre-boot.db
	SOLUTION: ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
	DOC PATH: Tasks -> Administer a Cluster -> Operating etcd clusters for Kubernetes


	Restore the original state of the cluster using the backup file.
	SOLUTION: Restore the backup:  ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db  (Restoring from same server and restoring in a different directory)
	Update the /etc/kubernetes/manifets/etcd.yaml.  Look for the volume name etcd-data.  Change the path to /var/lib/etcd-from-backup so it points to that now.  The pod is automatically re-created, being a static pod and placed under the /etc/kubernetes/manifests directory.  Also change the voulmeMounts mountPath to /var/lib/etcd-from-backup.
	TO GO THROUGH AGAIN....
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#restoring-an-etcd-cluster
	DOC PATH: Tasks -> Administer a Cluster -> Operating etcd clusters for Kubernetes
	
	

Backup and Restore Methods 2
	How many clusters are defined in the kubeconfig on the student-node?
	SOLUTION: kubectl config get-clusters
	DOC: https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/
	DOC PATH: Reference -> Command (kubectl) -> kubectl -> kubectl config
	
	Switch to cluster1
	SOLUTION: kubectl config use-context cluster1
	DOC: https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/
	DOC PATH: Reference -> Command (kubectl) -> kubectl -> kubectl config
	
	What is the IP address of the External ETCD datastore used in cluster2?
	SOLUTION: SSH to the node it should be on.  cluster2-controlplane.
	ps -ef | grep etcd
	
	How many nodes are part of the ETCD cluster that etcd-server is a part of?  Note: The ETCD cluster is an external ETCD server, not on the controlplane.
	SOLUTION:   SSH to the external etcd-server.  Find the endpoints and certs.
	
	ETCDCTL_API=3 etcdctl \
 --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/etcd/pki/ca.pem \
 --cert=/etc/etcd/pki/etcd.pem \
 --key=/etc/etcd/pki/etcd-key.pem \
  member list
  
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#securing-communication
	DOC PATH: Tasks -> Administer a Cluster -> Operating etcd clusters for Kubernetes
	
	
	Take a backup of etcd on cluster1 and save it on the student-node at the path /opt/cluster1.db
	SOLUTION: Inspect the endpoints and certs used by the etcd pod.  (kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls) & (kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep pki)
	
	In the controlplane of the cluser1, take a backup with the endpoints and certs that were given (ETCDCTL_API=3 etcdctl --endpoints=https://10.1.220.8:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster1.db)
	
	Copy backup to another node.
	scp controlplane:/opt/cluster1.db /optcluster1.db
	TO GO THROUGH AGAIN... INCLUDING RESTORES
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
	DOC PATH: Tasks -> Administer a Cluster -> Operating etcd clusters for Kubernetes
	
	
	
	
	
------------------
SECURITY
View Certificate Details
	Identify the certificate file used for the kube-api server.
	SOLUTION: cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for the line --tls-cert-file.
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server.
	SOLUTION: cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for the line etcd-certfile
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	Identify the key used to authenticate kubeapi-server to the kubelet server.
	SOLUTION: cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for the line kubelet-client-key
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	Identify the key used to authenticate kubeapi-server to the kubelet server.
	SOLUTION: cat /etc/kubernetes/manifests/etcd.yaml and look for the line cert-file
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	Identify the ETCD Server CA Root Certificate used to serve ETCD Server.  ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server.
	SOLUTION: cat /etc/kubernetes/manifests/etcd.yaml and look for the line trusted-ca-file
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	What is the Common Name (CN) configured on the Kube API Server Certificate?
	SOLUTION: openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text.  Look for Subject CN
	DOC: https://kubernetes.io/docs/setup/best-practices/certificates/
	DOC PATH: Getting Stated -> Best Practices -> PKI certificates and requirements
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/certificates/
	DOC PATH: Tasks -> Generate Certificates Manually

	What is the name of the CA who issued the Kube API Server Certificate?
	SOLUTION: openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text.  Look for Issuer CN
	DOC: https://kubernetes.io/docs/setup/best-practices/certificates/
	DOC PATH: Getting Stated -> Best Practices -> PKI certificates and requirements
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/certificates/
	DOC PATH: Tasks -> Generate Certificates Manually

	What is the Common Name (CN) configured on the ETCD Server certificate?
	SOLUTION: openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text.  Look for Subject CN
	DOC: https://kubernetes.io/docs/setup/best-practices/certificates/
	DOC PATH: Getting Stated -> Best Practices -> PKI certificates and requirements
	DOC: https://kubernetes.io/docs/tasks/administer-cluster/certificates/
	DOC PATH: Tasks -> Generate Certificates Manually
	
	Kubectl suddenly stops responding to your commands. Check it out! Someone recently modified the /etc/kubernetes/manifests/etcd.yaml file
	SOLUTION: The --cert-file path .crt file does not exist.  The path should be corrected.  Update the YAML to use /etc/kubernetes/pki/etcd/server.crt instead.  YAML - /etc/kubernetes/manifests/etcd.yaml
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details
	
	The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue.
	SOLUTION: Inspect the kube-apiserver.  crictl ps -a | grep kube-apiserver
	Inspect the logs - crictl logs --tail=2 <CONTAINERID>.
	You may get a handshake failed: tls: failed to verify certificate kind of error.
	Go to the YAML and correct it.  /etc/kubernetes/manifests/kube-apiserver.yaml
	In the YAML, use /etc/kubernetes/pki/etcd/ca.crt for the ETCD CA Certificate.
	DOC: https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#constants-and-well-known-values-and-paths
	DOC PATH: Reference -> Setup Tools -> Kubeadm -> Implementation details

Certificates API
	Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file
	SOLUTION: Generate a base64 format for the .csr.  Then create a YAML and use that base64 in that.
	DOC: https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest
	DOC PATH: Reference -> API Access Control -> Certificates and Certificate Signing Request
	YAML:
		---
		apiVersion: certificates.k8s.io/v1
		kind: CertificateSigningRequest
		metadata:
  		  name: akshay
		spec:
  		  groups:
  		  - system:authenticated
  		  request: <Paste the base64 encoded value of the CSR file>
  		  signerName: kubernetes.io/kube-apiserver-client
  		  usages:
  		  - client auth

	What is the Condition of the newly created Certificate Signing Request object?
	SOLUTION: kubectl get csr
	DOC: https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#approve-certificate-signing-request
	DOC PATH: Reference -> API Access Control -> Certificates and Certificate Signing Request
	
	Approve the CSR Request
	SOLUTION: kubectl certificate approve akshay
	DOC: https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#approve-certificate-signing-request
	DOC PATH: Reference -> API Access Control -> Certificates and Certificate Signing Request
	
	You are not aware of a request coming in.  What groups is this CSR requesting access to?
	SOLUTION:  Look at the YAML of this CSR using -o yaml.  kubectl get csr <CSR-NAME> -o yaml.
	

KubeConfig
	Where is the default kubeconfig file located in the current environment?
	SOLUTION: /root/.kube/config
	
	What is the current context set to in the ‘my-kube-config’ file?
	SOLUTION: kubectl config current-context --kubeconfig my-kube-config 
	
	I would like to use the ‘dev-user’ to access ‘test-cluster-1’.  Set the current context to the right one so I can do that.  Use the kubectl config-use-context command
	SOLUTION: Use that context, using the ‘my-kube-config’ file.   kubectl config --kubeconfig=/root/my-kube-config use-context research
	To know the current context -> kubectl config --kubeconfig=/root/my-kube-config current-context
	
	Set up the my-kube-config file as the default kubeconfig by overwriting the content of ~/.kube/config with the content of the my-kube-config file.
	SOLUTION: Copy over the file to .kube/config.  cp my-kube-config ~/.kube/config
	
	With the current-context set to ‘research’, we are trying to access the cluster.  However something seems to be wrong.  Fix the issue.  All user certs are located in /etc/kubernetes/pki/users
	SOLUTION:  Check the config in ~/.kube/config.  The client-certificate is wrong for dev-user.  vi ~/.kube/config.

Role Based Access Controls
	Inspect the environment and identify the authorization modes configured on the cluster.  Check the kube-apiserver settings.  The 
	SOLUTION:  Get pods and find kube-apiserver-controlplane.  kubectl describe pod kube-apiserver-controlplane -n kube-system.  Look for --authorization-mode.
	
	Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace
	Role: developer
	Role Resources: pods
	Role Actions: list
	Role Actions: create
	Role Actions: delete
	RoleBinding: dev-user-binding
	RoleBinding: Bound to dev-user
	SOLUTION: Create a role.  Imperative Command.  kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods
	Create a rolebinding.  Imperative Command.  kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
	
	Add a new rule in the existing role ‘developer’ to grant the ‘dev-user’ permissions to create deployments in the ‘blue’ namespace
	SOLUTION: Edit the developer role.  kubectl edit role developer -n blue.  Add the apiGroups section to the bottom of the YAML
	YAML:
		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
  		  name: developer
  		  namespace: blue
		rules:
		- apiGroups:
  		  - apps
  		  resourceNames:
  		  - dark-blue-app
  		  resources:
  		  - pods
  		  verbs:
  		  - get
  		  - watch
  		  - create
  		  - delete
		- apiGroups:
  		  - apps
  		  resources:
  		  - deployments
  		  verbs:
  		  - create
	
	
Cluster Roles
	‘michelle’ needs a ClusterRole and ClusterRoleBindings created to allow her to access storage.  You can get API groups and resource names from the command ‘kubectl api-resources’.
	ClusterRole: storage-admin
	Resource: persistentvolumes
	Resource: storageclasses
	ClusterRoleBinding: michelle-storage-admin
	ClusterRoleBinding Subject: michelle
	ClusterRoleBinding Role: storage-admin
	SOLUTION:  Create Yaml(s) for these roles.  Apply the yaml afterward
	YAML:
		---
		kind: ClusterRole
		apiVersion: rbac.authorization.k8s.io/v1
		metadata:
  		  name: storage-admin
		rules:
		- apiGroups: [""]
  		  resources: ["persistentvolumes"]
  		  verbs: ["get", "watch", "list", "create", "delete"]
		- apiGroups: ["storage.k8s.io"]
  		  resources: ["storageclasses"]
  		  verbs: ["get", "watch", "list", "create", "delete"]
		---
		kind: ClusterRoleBinding
		apiVersion: rbac.authorization.k8s.io/v1
		metadata:
  		  name: michelle-storage-admin
		subjects:
		- kind: User
  		  name: michelle
  		  apiGroup: rbac.authorization.k8s.io
		roleRef:
  		  kind: ClusterRole
  		  name: storage-admin
  		  apiGroup: rbac.authorization.k8s.io

Service Accounts
	Inspect the Dashboard Application POD and identify the Service Account mounted on it.
	SOLUTION: View the yaml, look for service account.  kubectl get pod web-dashboard -o yaml.  
	
	At what location is the Service Account Credentials available within the pod?
	SOLUTION: Describe the pod.  Look for the Mount for the SA.  kubectl describe pod web-dashboard
	
	The application needs a ServiceAccount with the right permissions to be created to authenticate to Kubernetes.  The ‘default’ ServiceAccount has limited access.  Create a new ServiceAccount named ‘dashboard-sa’
	SOLUTION: kubectl create serviceaccount dashboard-sa
	
	Create an authorization token for the newly created service account.  Copy the generated token and taste it into the token field of the UI of the terminal dashboard.
	SOLUTION: kubectl create token dashboard-sa
	
	You shouldn’t have to copy and paste the token each time.  The Dashboard application is programmed to read token from the secret mount location.  However, the ‘default’ service account is mounted.  Update the deployment to use the newly created ServiceAccount.
	SOLUTION: Use the kubectl set command to set the Service Account to the deployment.  kubectl set serviceaccount deploy/web-dashboard dashboard-sa.

Image Security
	Create a secret object with the credentials required to access the registry.
	Name: private-reg-cred
	Username: dock_user
	Password: dock_password
	Server: myprivateregistry.com:5000
	Email: dock_user@myprivateregistry.com
	Secret: private-reg-cred
	Secret Type: docker-registry
	Secret Data
	SOLUTION: Use an imperative command.  kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
	
	Configure the deployment to use credentials from the new secret to pull images from the private registry
	SOLUTION: Edit the deployment and in the spec section with the image, add the imagePullSecrets form the YAML.  imagePullSecrets is at the same level as containers, not inside.
	YAML:
		spec:
      	  containers:
      	  - image: myprivateregistry.com:5000/nginx:alpine
        	imagePullPolicy: IfNotPresent
        	name: nginx
        	resources: {}
        	terminationMessagePath: /dev/termination-log
        	terminationMessagePolicy: File
          imagePullSecrets:
          - name: private-reg-cred

	
Security Contexts
	What is the user used to execute the sleep process within the ubuntu-sleeper pod?
	SOLUTION: kubectl exec ubuntu-sleeper -- whoami
	
	Edit the pod ubuntu-sleeper to run the sleep process with the userID 1010.  Do not modify the name or image of the pod.
	SOLUTION: Delete the pod and re-create the YAML adding the security context as user.
	YAML:
		---
		apiVersion: v1
		kind: Pod
		metadata:
  		  name: ubuntu-sleeper
  		  namespace: default
		spec:
  		  securityContext:
    		runAsUser: 1010
  		  containers:
  		  - command:
    		- sleep
    		- "4800"
    		image: ubuntu
    		name: ubuntu-sleeper
	
	Update pod ‘ubuntu-sleeper’ to run as root and with the SYS_TIME capability
	SOLUTION: Delete the ubnutu-sleeper pod.  Create a pod/YAML
	YAML:
		---
		apiVersion: v1
		kind: Pod
		metadata:
 		 name: ubuntu-sleeper
  		 namespace: default
		spec:
  		  containers:
  		  - command:
    		- sleep
    		- "4800"
    		image: ubuntu
    		name: ubuntu-sleeper
    		securityContext:
      		  capabilities:
        		  add: ["SYS_TIME"]
	
	
Network Policies
	Create a network policy to allow traffice from Internal application only to the ‘payroll-service’ and ‘db-service’.  Ensure that you allow egress traffice to DNS ports TCP and UDP (port 53) to enable DNS resolution from the internal port.  Using these specs, you might want to enable ingress traffic.
	Policy Name: internal-policy
	Policy Type: Egress
	Egress Allow: payroll
	Payroll Port: 8080
	Egress Allow: mysql
	MySQL Port: 3306
	SOLUTION: Create a YAML
	YAML:
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
  		  name: internal-policy
 		  namespace: default
		spec:
  		  podSelector:
    		matchLabels:
     		  name: internal
  		  policyTypes:
  		  - Egress
  		  - Ingress
  		  ingress:
  		    - {}
  		  egress:
  		  - to:
    		- podSelector:
        		matchLabels:
          		  name: mysql
    		ports:
    		- protocol: TCP
      		  port: 3306
      		  
  		- to:
    	  - podSelector:
        		matchLabels:
          		  name: payroll
    	  ports:
    	  - protocol: TCP
      		port: 8080

  		- ports:
    	  - port: 53
      	 	protocol: UDP
    	  - port: 53
      		protocol: TCP

------------------
STORAGE
Persistent Volume Claims
	The ‘webapp’ pod stores logs at /log/app.log.  View the logs
	SOLUTION:  kubectl exec webapp -- cat /log/app.log
	
	Configure a volume to store these logs at /var/log/webapp on the host.
	Name: webapp
	Image Name: kodekloud/event-simulator
	Volume HostPath: /var/log/webapp
	Volume Mount: /log
	SOLUTION: Delete the pod and recreate it.  Create a YAML
	YAML:
		apiVersion: v1
		kind: Pod
		metadata:
  		  name: webapp
		spec:
  		  containers:
  		  - name: event-simulator
    		image: kodekloud/event-simulator
   		    env:
    	    - name: LOG_HANDLERS
      		  value: file
    		volumeMounts:
    		- mountPath: /log
      		  name: log-volume

  		  volumes:
  		  - name: log-volume
    		hostPath:
      		  # directory location on host
      		  path: /var/log/webapp
      		  # this field is optional
      		  type: Directory


	Create a Persistent Volume with the given speculation
	Volume Name: pv-log
	Storage: 100Mi
	Access Modes: ReadWriteMany
	Host Path: /pv/log
	Reclaim Policy: Retain
	SOLUTION:  Create a YAML
	YAML:
		apiVersion: v1
		kind: PersistentVolume
		metadata:
  		  name: pv-log
		spec:
  		  persistentVolumeReclaimPolicy: Retain
  		  accessModes:
    		- ReadWriteMany
  		  capacity:
    		storage: 100Mi
  		  hostPath:
    		 path: /pv/log
    		 
	Let us claim some of that storage for our application.  Create a PVC with the given specs
	Volume Name: claim-log-1
	Storage Request: 50Mi
	Access Modes: ReadWriteMany
	SOLUTION: Create a YAML
	YAML:
		kind: PersistentVolumeClaim
		apiVersion: v1
		metadata:
 		  name: claim-log-1
		spec:
  		  accessModes:
    		- ReadWriteMany
  		  resources:
    		requests:
      		  storage: 50Mi
      		  
	Update the ‘webapp’ pod to use the persistent volume claim as its storage.
	Name: webapp
	Image Name: kodekloud/event-simulator
	Volume: PersistentVolumeClaim=claim-log-1
	Volume Mount: /log
	SOLUTION:  Replace hostPath configured with a new created PersistentVolumeClaim.  You can create a new YAML with this solution
	YAML:
		apiVersion: v1
		kind: Pod
		metadata:
  		  name: webapp
		spec:
  		  containers:
  		  - name: event-simulator
    		image: kodekloud/event-simulator
    		env:
    	    - name: LOG_HANDLERS
      		  value: file
    	    volumeMounts:
    	    - mountPath: /log
      		  name: log-volume
  		volumes:
  		- name: log-volume
    	  persistentVolumeClaim:
      		claimName: claim-log-1
	


Storage Class
	Create a new PVC by the name of local-pvc that should bind to the volume ‘local-pv’.
	PVC: local-pvc
	Correct Access Mode?
	Correct StorageClass Used?
	PVC requests volume size = 500Mi?
	SOLUTION: Create a YAML
	YAML:
		---
		kind: PersistentVolumeClaim
		apiVersion: v1
		metadata:
  		  name: local-pvc
		spec:
  		  accessModes:
  		  - ReadWriteOnce
  		  storageClassName: local-storage
  		  resources:
    		requests:
      		  storage: 500Mi


	Create a new pod called nginx with the image nginx:alpine.  The Pod should make use of the PVC ‘local-pvc’ and mount the volume at the path /var/www/html.
	SOLUTION:  Create a YAML
	YAML:
		---
		apiVersion: v1
		kind: Pod
		metadata:
 		  name: nginx
 		  labels:
    		name: nginx
		spec:
  		  containers:
  		  - name: nginx
    		image: nginx:alpine
    		volumeMounts:
      		  - name: local-persistent-storage
        		mountPath: /var/www/html
  		  volumes:
    		- name: local-persistent-storage
      		  persistentVolumeClaim:
        		claimName: local-pvc


	Create a new Storage Class called ‘delayed-volume-sc’ that makes use of the below specs
	provisioner: kubernetes.io/no-provisioner
	volumeBindingMode: WaitForFirstConsumer
	SOLUTION: Create a YAML
	YAML:
		---
		apiVersion: storage.k8s.io/v1
		kind: StorageClass
		metadata:
  		  name: delayed-volume-sc
		provisioner: kubernetes.io/no-provisioner
		volumeBindingMode: WaitForFirstConsumer

-------------------
NETWORKING
Explore Environment
	If you were to ping google from the controlplane node, which route does it take?  What is the default gateway?
	SOLUTION:  ip route show default
	
	What is the port kube-scheduler is listening on in the controlplane node
	SOLUTION: netstat -nplt | grep scheduler

CNI
	Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes
	SOLUTION: ps -aux | grep kubelet | grep --color container-runtime-endpoint.  Look for --container-runtime-endpoint.
	
	What is the path configured with all binaries of CNI supported plugins
	SOLUTION: The binaries are in /opt/cni/bin by default
	
	What is the CNI Plugin configured to be used on this kubernetes cluster
	SOLUTION: ls /etc/cni/net.d.  Identify the name of the plugin.

Deploy Network Solution
	Deploy weave-net networking solution to the cluster.  The manifest yaml is in /root/weave
	SOLUTION:  kubectl apply -f weave-daemonset-k8s.yaml (this is in /root/weave)
	
	
Networking Weave
	What is the networking solution used by this cluster?
	SOLUTION: Check the config (/etc/cni/net.d)

	What is the POD IP address range configured by ‘weave’?
	SOLUTION: ip addr show weave
	
	What is the default gateway configured on the PODs scheduled on node01?
	SOLUTION: ssh to node01.  ‘ip route’


Service Networking
	What type of proxy is the ‘kube-proxy’ configured to use?
	SOLUTION: Check the logs of the kube-proxy.  ‘kubectl logs <kube-proxy-pod> -n kube-system


CoreDNS in Kubernetes
	Where is the configuration file located for configuring the CoreDNS service?
	SOLUTION: kubectl -n kube-system describe deployments.apps coredns | grep -A2 Args | grep Corefile	
	How is the Corefile passed into the CoreDNS POD?
	SOLUTION: kubectl get cm -n kube-system
	
	What is the root domain/zone configured for this kubernetes cluster?
	SOLUTION: kubectl describe configmap coredns -n kube-system
	
	What name can be used to access the ‘hr’ web server from the ‘test’ application?
	SOLUTION: kubectl get svc.  Note the name of the service.

	We just deployed a web server ‘webapp’ that accesses a database ‘mysql’ server.  However the web server is failing to connect to the database server.  Troubleshoot and fix the issue.
	SOLUTION:  Change the hostname on DB_HOST in the deployment.  ‘kubectl edit deploy webapp’
	Correct the DB_HOST to mysql.payroll.
	
	From the ‘hr’ pod ‘nslookup’ the ‘mysql’ service and redirect the output to a file ‘/root/CKA/nslookup.out’
	SOLUTION: kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out


CKA - Ingress Networking - 1
	Which namespace is the Ingress Resource deployed in?
	SOLUTION: kubectl get ingress --all-namespaces
	
	You are requested to make a new application available at /pay.
	Ingress Created
	Path: /pay
	Configure correct backend service
	Configure correct backend port
	SOLUTION: Usually you’d edit the ingress-wear-watch resource.  But this is in a different namespace.  Create a new YAML for an ingress resource
	YAML:
		---
		apiVersion: networking.k8s.io/v1
		kind: Ingress
		metadata:
  		  name: test-ingress
  		  namespace: critical-space
  		  annotations:
    		nginx.ingress.kubernetes.io/rewrite-target: /
    		nginx.ingress.kubernetes.io/ssl-redirect: "false"
		spec:
  		  rules:
  		  - http:
      			paths:
      			- path: /pay
        		  pathType: Prefix
        		  backend:
          			service:
           				name: pay-service
           				port:
            			  number: 8282
		

CKA - Ingress Networking - 2
	LOOK AT INGRESS CONTROLLER CREATION
	
	Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.  Also, make use of the ‘rewrite-target’ annotation field - ‘nginx.ingress.kubernetes.io/rewrite-target: /‘
	Ingress Created
	Path: /wear
	Path: /watch
	Configure correct backend service for /wear
	Configure correct backend service for /watch
	Configure correct backend port for /wear service
	Configure correct backend port for /watch service
	SOLUTION: Create a YAML
	YAML:
		---
		apiVersion: networking.k8s.io/v1
		kind: Ingress
		metadata:
  		  name: ingress-wear-watch
  		  namespace: app-space
  		  annotations:
    		nginx.ingress.kubernetes.io/rewrite-target: /
    		nginx.ingress.kubernetes.io/ssl-redirect: "false"
		spec:
 		  rules:
  		  - http:
      		  paths:
      		  - path: /wear
        		pathType: Prefix
        		backend:
          		  service:
           			name: wear-service
           			port: 
            		  number: 8080
      		  - path: /watch
        		pathType: Prefix
        		backend:
          		  service:
           			name: video-service
           			port:
            		  number: 8080

------------------
DESIGN AND INSTALL A KUBERNETES CLUSTER

-----------------
INSTALL “KUBERNETES THE KUBEADM WAY”
Deploy a Kubernetes Cluster Using Kubeadm
-----------------
TROUBLESHOOTING
Application Failure
	LOOK AT PODS/ENV VARIABLES/SERVICES/LABELS/PORT NUMBERS.  MAKE SURE THEY ALL SYNC UP.
	A simple tier 2 application is deployed in the ‘alpha’ namespace.  It must display a green web page on success.  You can see the failure on the web terminal.  Troubleshoot the issue.  The pod is mysql, the service is mysql-service, the deployment is webapp-mysql.  The web service is web-service.
	SOLUTION:  You may notice getting services, kubectl get svc -n alpha.  There is only a ‘mysql’ service.  If you do a kubectl get deploy webapp-mysql -n alpha -o yaml, you’ll see the DB_HOST is mysql-service.  This should be corrected.  Delete the service ‘mysql’ and create a new service that shows ‘mysql-service’
	YAML:
		---
		apiVersion: v1
		kind: Service
		metadata:
  		  name: mysql-service
  		  namespace: alpha
		spec:
   	 	  ports:
    	  - port: 3306
      		targetPort: 3306
    	  selector:
      		name: mysql 
      		
	The same tier 2 application is deployed in ‘beta’ namespace.  It is failed again. 
	SOLUTION:  Look at the service for ‘mysql-service’, the targerPort should be 3306 based off the diagram.  It is set to 8080.  Update the YAML.  kubectl edit svc -n beta mysql-service.  Change the targetPort to 3306.
	
	The same tier 2 application is deployed in the ‘gamma’ namespace.  It has failed again.
	SOLUTION:  Look at both the ‘mysql-service’ service and the ‘mysql’ pod.  You may notice the labels are off.  mysql-service -> selector: name=sql00001.  mysql (pod) -> selector: name=mysql
	Update the ‘mysql-service’ service to change the label.


Control Plane Failure
	The cluster is broken.  We tried deploying an application but it’s not working.  Troubleshoot and fix.
	SOLUTION:  You’ll notice the kube-scheduler pod is erroring out.  Under describe for the pod, you’ll see back-off restarting failed container kube-scheduler in pod.  Go to the manifests and make sure it looks ok.  Manifests -> /etc/kubernetes/manifests.  Check for typos
	
	
	Scale the deployment ‘app’ to 2 pods
	SOLUTION: kubectl scale deploy app --replicas=2
	
	
	Another break.  Tried scaling the deployment to 3 replicas.  But it’s not happening
	SOLUTION: Look at the failing pod.  Check out the YAML.  kubectl get pod pod-name -o yaml
	Look over hostpaths.  Make sure they look correct.  One has a ‘WRONG-PKI-DIRECTORY’ in it’s path.  Because this is a relicaset/deployment, check out the manifests yaml and change it from there.  This is the kube-controller manifest.

Worker Node Failure
	Fix the broken cluster
	SOLUTION: Go to a failing node and check the status of kubectl.  Start it, if necessary.
	
	Fix the cluster again.  
	SOLUTION:  Go to the failing node.  kubectl is failed, but not found.  You can try updating apt-get and installing kubectl, but look at the config file for kubectl.  /var/lib/kubelet/config.yaml.  Make sure everything looks ok here.  In this scenario, a directory was wrong for a cert.
	

Troubleshoot Network
	A simple tier 2 application is deployed in the ‘triton’ namespace’.  The application is currently failed.  It has a ‘mysql’ pod, ‘mysql’ service’, a ‘webapp-mysql’ deployment, and a ‘web-service’ service.
	SOLUTION: The describe for the pod ‘mysql’ suggests that weave it not installed.  Go ahead and install it.  https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network.  To apply weave -> kubectl apply -f https://reweave.azurewebsites.net/k8s/v1.29/net.yaml.
	
	Another network issue.  Same parameters.  Get Pods shows that kube-proxy is not starting.
	SOLUTION:  Check out the logs.  kubectl logs -n kube-system <kube-proxy-pod-name>.  Notice that /var/lib/kube-proxy/configuration.conf is missing.  Also check out the configmap for kube-proxy.  kubectl -n kube-system describe configmap kube-proxy.  You’ll see the file there is listed as /var/lib/kube-proxy/kubeconfig.conf.  The daemonset and the configmap appear to be wrong.  Edit the daemonset using kubectl get ds -n kube-system kube-proxy -o yaml > em.yaml
	Change the config line to --config=/var/lib/kube-proxy/config.conf

-----------------
OTHER TOPICS
Advanced Kubectl Commands
	Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json
	SOLUTION: kubectl get nodes -o json > /opt/outputs/nodes.json
	
	Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt
	SOLUTION: kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt
	
	Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
	SOLUTION: kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt
	
	A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
	SOLUTION: kubectl config view --kubeconfig=my-kube-config  -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt
	
	A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt
	SOLUTION: kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt
	
	That was good, but we don't need all the extra details. Retrieve just the first 2 columns of output and store it in /opt/outputs/pv-and-capacity-sorted.txt
	SOLUTION: kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt
	
	Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name
	SOLUTION: kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
	
